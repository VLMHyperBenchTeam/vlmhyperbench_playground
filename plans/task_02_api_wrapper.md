# Задача 2: Реализация API Wrapper (FastAPI)

## Цель
Создать унифицированный HTTP-интерфейс (OpenAI-compatible) для взаимодействия с различными инференс-движками (vLLM, SGLang, HF) внутри Docker-контейнера модели.

## Контекст
См. [ADR-003: API Wrapper для инференса](../docs_site/docs/architecture/adr/003-api-wrapper.md).
Необходимо абстрагировать детали конкретных бэкендов и обеспечить централизованный сбор метрик производительности.

## Подзадачи

1.  **FastAPI Application**:
    *   Создать базовое приложение FastAPI.
    *   Реализовать эндпоинт `POST /v1/chat/completions`.
    *   Поддержать `model_params` в теле запроса (для передачи специфичных параметров, игнорируемых стандартным OpenAI API, но важных для наших моделей).

2.  **Telemetry Middleware**:
    *   Реализовать middleware для замера времени выполнения запроса (Latency).
    *   Интегрировать `pynvml` (или аналог) для замера потребления VRAM до и после запроса.
    *   Добавлять метрики в заголовки ответа (`X-Latency-Ms`, `X-Peak-Memory-Mb`).

3.  **Backend Adapters**:
    *   **AbstractBackend**: Базовый класс для адаптеров.
    *   **vLLMAdapter**: Реализация через `vllm.AsyncLLMEngine`.
    *   **HuggingFaceAdapter**: Реализация через `transformers` pipeline (с поддержкой `asyncio` через `run_in_executor` или нативные асинхронные методы, если есть).
    *   **SGLangAdapter**: (Опционально на первом этапе) Проксирование к локальному серверу SGLang.

4.  **Watchdog (Resource Monitoring)**:
    *   Фоновый процесс, периодически (раз в 100мс) опрашивающий системные метрики (GPU Util, CPU RAM).
    *   Сохранение статистики для последующего анализа.

## Ожидаемый результат
*   Python-пакет `packages/api_wrapper`.
*   Запускаемый сервер (через `uvicorn`), принимающий запросы в формате OpenAI Chat Completions.
*   Ответы содержат сгенерированный текст и метаданные о производительности.