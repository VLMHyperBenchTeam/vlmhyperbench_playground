# Исследование архитектуры EvalScope

## 1. Обзор
**EvalScope** — это фреймворк для оценки производительности и качества моделей (LLM, VLM), разработанный в экосистеме ModelScope. Он позиционируется как универсальный инструмент, поддерживающий различные бэкенды оценки.

## 2. Архитектура (Overall Architecture)
Архитектура состоит из трех основных слоев:

### 2.1. Input Layer (Входной слой)
*   **Model Sources**: Поддержка как API-моделей (OpenAI, DashScope), так и локальных моделей (через ModelScope/HuggingFace).
*   **Datasets**: Стандартные бенчмарки (MMLU, GSM8k, OCRBench) и пользовательские датасеты (Custom Data).
*   **Data Adapter**: Модуль для конвертации входных данных в унифицированный формат фреймворка.

### 2.2. Core Functions (Ядро)
*   **Multi-backend Evaluation**: EvalScope не изобретает велосипед, а выступает как мета-фреймворк, делегируя выполнение специализированным бэкендам:
    *   **Native**: Собственный движок для простых задач.
    *   **OpenCompass**: Для текстовых LLM.
    *   **VLMEvalKit**: Специализированный бэкенд для VLM (поддерживает мультимодальные задачи).
    *   **RAGAS**: Для оценки RAG-систем.
*   **Performance Monitoring**: Инструменты для замера скорости инференса (TTFT, TPOP, Latency).
*   **Model Adapter**: Унифицирует интерфейс взаимодействия с различными моделями.

### 2.3. Output Layer (Выходной слой)
*   **Structured Reports**: Генерация отчетов в JSON, CSV, Markdown.
*   **Visualization**: Интеграция с Gradio, WandB, SwanLab для визуализации результатов.

## 3. Ключевые идеи для VLMHyperBench

### 3.1. Использование VLMEvalKit как бэкенда
EvalScope использует **VLMEvalKit** для оценки VLM. Это подтверждает, что для VLM задач имеет смысл не писать все с нуля, а интегрировать существующие мощные инструменты, либо использовать их подходы.

### 3.2. Registry-based Design (Version 1.0 Refactoring)
В версии 1.0 EvalScope перешел на реестровую систему компонентов (Benchmarks, Metrics). Это позволяет легко добавлять новые компоненты через декораторы или конфиги, что перекликается с нашей идеей `ModelRegistry` и `BenchmarkRunConfig`.

### 3.3. Performance Evaluation Tool
EvalScope имеет отдельный модуль для стресс-тестирования инференса. В VLMHyperBench это также важная часть (замер скорости/памяти), которую стоит выделить в отдельный этап или метрику.

### 3.4. Абстракция Model Adapter
Наличие `Model Adapter` позволяет унифицировать вызовы к разным моделям (локальным, API). Это подтверждает правильность нашего подхода с `ModelInterface`.

## 4. Сравнение с VLMHyperBench
| Характеристика | EvalScope | VLMHyperBench (Prototype) |
| :--- | :--- | :--- |
| **Цель** | Универсальная оценка всего (LLM, VLM, RAG) | Специализация на VLM и документах (OCR, VQA) |
| **Изоляция** | Зависит от бэкенда (часто в одном env) | Строгая изоляция через Docker/Singularity |
| **Инференс** | Абстрагирован через адаптеры | Абстрагирован через ModelInterface (HF/vLLM) |
| **Конфигурация** | Python/YAML | CSV/JSON (User Config) |
| **Pipeline** | Монолитный (в рамках одного запуска) | Модульный (Stages в разных контейнерах) |

## 5. Вывод
EvalScope — отличный пример мета-фреймворка. Однако VLMHyperBench имеет уникальное преимущество в **строгой изоляции окружения** (Docker-per-model) и специализации на **документарных задачах**, что делает его более подходящим для production-оценки в корпоративных контурах, где важна воспроизводимость и безопасность зависимостей.