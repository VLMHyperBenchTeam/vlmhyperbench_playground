# Задача 4: Асинхронный Inference Loop

## Цель
Перевести основной скрипт исполнения `run_vlm.py` на асинхронную модель работы для повышения пропускной способности и утилизации ресурсов.

## Контекст
Текущая реализация `run_vlm.py` работает синхронно: загружает изображение, отправляет запрос, ждет ответа, сохраняет. При использовании внешних API или локального сервера модели это создает простои.
Асинхронность позволит отправлять пакеты запросов параллельно (контролируемая конкурентность).

## Подзадачи

1.  **Async HTTP Client**:
    *   Заменить прямые вызовы модели на HTTP-запросы к локальному API Wrapper (через `httpx`).
    *   Реализовать класс `AsyncModelClient`.

2.  **Рефакторинг `run_vlm.py`**:
    *   Переписать функцию `run_vlm_stage_greet` и основной цикл на `async`/`await`.
    *   Использовать `asyncio.Semaphore` для ограничения количества одновременных запросов (чтобы не перегрузить модель/память).

3.  **Интеграция PromptManager**:
    *   В цикле обработки датасета вызывать `PromptManager.resolve_prompt` для каждого элемента.

4.  **Обработка ошибок и Retry**:
    *   Добавить логику повторных попыток при сетевых ошибках или 5xx ответах от API Wrapper.
    *   Логировать неуспешные запросы для последующего анализа.

## Ожидаемый результат
*   Скрипт `run_vlm.py` запускает `uvicorn` с API Wrapper (в фоне) или ожидает его готовности.
*   Обработка датасета происходит асинхронно.
*   Скорость прогона (items/sec) выше, чем в синхронной версии.