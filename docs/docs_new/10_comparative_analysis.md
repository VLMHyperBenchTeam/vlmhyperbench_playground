# Сравнительный анализ: VLMHyperBench vs Существующие решения

Этот документ содержит анализ существующих подходов к оценке моделей и обоснование выбранной архитектуры VLMHyperBench.

## 1. Сравнение с EvalScope (ModelScope)

**EvalScope** — наиболее близкий по духу фреймворк, предоставляющий унифицированный интерфейс для оценки LLM и VLM.

| Характеристика | EvalScope / VLMEvalKit | VLMHyperBench |
| :--- | :--- | :--- |
| **Архитектура** | Монолитная / Библиотечная. Зависимости устанавливаются в одно окружение. | **Микросервисная / Контейнерная**. Каждый этап изолирован в своем контейнере. |
| **Управление зависимостями** | Пользователь сам разрешает конфликты версий (CUDA, Torch). | **EnvManager** автоматически создает изолированные среды под каждую модель. |
| **Фокус** | Универсальность (LLM, VLM, RAG, Agents). | Специализация на **VLM и Document Understanding** (OCR, DocVQA). |
| **Метрики производительности** | Базовые. | **Глубокое профилирование** (TTFT, Memory Peak, Latency) через API Wrapper. |
| **Интеграция** | Тесная интеграция с экосистемой ModelScope. | **Агностичность**. Поддержка HF, vLLM, SGLang, OpenAI API. |
| **Сценарий использования** | Исследования, быстрые эксперименты. | **Production-grade оценка**, CI/CD пайплайны, корпоративные контуры. |

### Почему не использовать EvalScope "как есть"?
Хотя EvalScope мощный инструмент, его архитектура "все в одном" плохо подходит для промышленного бенчмаркинга сотен моделей, где конфликты зависимостей неизбежны. VLMHyperBench решает эту проблему через строгую изоляцию. Однако, мы можем использовать **компоненты** EvalScope (например, реализации метрик или датасетов) внутри наших контейнеров.

## 2. Сравнение с LM-Evaluation-Harness

**LM-Evaluation-Harness** — стандарт де-факто для оценки LLM.

*   **Ограничения для VLM**: Изначально создан для текстовых моделей. Поддержка мультимодальности ограничена.
*   **Архитектура**: Библиотека, запускаемая локально. Не предоставляет инструментов для оркестрации Docker-контейнеров.

## 3. Сравнение с OpenCompass

**OpenCompass** — мощная платформа для оценки от OpenMMLab.

*   **Плюсы**: Огромная база поддерживаемых моделей и датасетов.
*   **Минусы**: Очень сложная конфигурация и высокий порог входа. Ориентирован на экосистему OpenMMLab.
*   **VLMHyperBench**: Предлагает более простой подход "Configuration as Code" (CSV) и фокусируется на легкой расширяемости через микро-пакеты.

## Выводы

VLMHyperBench занимает уникальную нишу:
1.  **Инфраструктурный слой**: Это не просто библиотека метрик, а *система управления бенчмарками* (Orchestrator), которая берет на себя управление Docker-контейнерами и окружениями.
2.  **Специализация**: Глубокая поддержка задач, связанных с документами (OCR, классификация, извлечение сущностей), что часто игнорируется в универсальных бенчмарках.
3.  **Безопасность и Стабильность**: За счет изоляции, падение одной модели или конфликт библиотек не ломает весь процесс оценки.