# –ù–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd  # type: ignore
from bench_utils.utils import get_run_id, load_config  # type: ignore

HEADER = "# üìù –û—Ç—á—ë—Ç –ø–æ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"


def _metrics_row_to_md(metrics: Dict[str, float]) -> str:
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫—É markdown-—Ç–∞–±–ª–∏—Ü—ã –∏–∑ –º–µ—Ç—Ä–∏–∫."""
    return (
        f"| {metrics.get('accuracy', 0):.4f} | {metrics.get('f1', 0):.4f} | "
        f"{metrics.get('precision', 0):.4f} | {metrics.get('recall', 0):.4f} |"
    )


def _df_to_md_table(df: pd.DataFrame, include_index: bool = True) -> str:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç DataFrame –≤ markdown-—Ç–∞–±–ª–∏—Ü—É —Ñ–æ—Ä–º–∞—Ç–∞ GitHub.

    –ï—Å–ª–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å ``tabulate`` –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, –≤—ã–≤–æ–¥–∏—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –∏
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–ø—Ä–æ—â—ë–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ``DataFrame.to_string``.
    """
    try:
        # –û–∫—Ä—É–≥–ª—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ 4 –∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏
        df_formatted = df.copy()
        float_cols = df_formatted.select_dtypes(include="number").columns
        df_formatted[float_cols] = df_formatted[float_cols].round(4)

        return df_formatted.to_markdown(index=include_index, tablefmt="github")  # type: ignore[attr-defined]
    except ImportError:
        warnings.warn("–ü–∞–∫–µ—Ç 'tabulate' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –≤–∏–¥–µ. "
                      "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ 'tabulate' –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö Markdown-—Ç–∞–±–ª–∏—Ü: pip install tabulate", stacklevel=2)
        return "```\n" + df.to_string(index=include_index) + "\n```"


def _append_md_section(lines: List[str], title: str) -> None:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤—Ç–æ—Ä–æ–≥–æ —É—Ä–æ–≤–Ω—è –≤ markdown."""
    lines.extend(["", f"## {title}", ""])


def build_report(config_path: Path, output_path: Path) -> None:
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –æ—Ç—á—ë—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ `check_classifiication.py`.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        config_path: –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É JSON, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–º—É –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –æ—Ü–µ–Ω–∫–∏.
        output_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ markdown-—Ñ–∞–π–ª–∞.
    """
    config = load_config(str(config_path))

    task_cfg = config["task"]
    model_cfg = config["model"]
    document_classes: Dict[str, str] = config["document_classes"]

    prompt_path = task_cfg.get("prompt_path")

    md_lines: List[str] = [HEADER, ""]

    # --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞—á–∏ ---
    _append_md_section(md_lines, "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞—á–∏")
    md_lines.append(f"* **–î–∞—Ç–∞—Å–µ—Ç:** `{task_cfg['dataset_path']}`")
    md_lines.append(f"* **–ü—Ä–æ–º–ø—Ç:** `{prompt_path}`")
    md_lines.append(
        f"* **Subsets:** {', '.join(task_cfg.get('subsets', []))}")
    if task_cfg.get("sample_size"):
        md_lines.append(f"* **Sample size:** {task_cfg['sample_size']}")

    # --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ ---
    _append_md_section(md_lines, "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")
    md_lines.append(f"* **–ú–æ–¥–µ–ª—å:** `{model_cfg['model_name']}`")
    # –í—ã–≤–æ–¥–∏–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ (–∫—Ä–æ–º–µ –∏–º–µ–Ω–∏)
    for key, value in model_cfg.items():
        if key == "model_name":
            continue
        md_lines.append(f"* **{key}:** {value}")

    # --- –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø—Ä–æ–º–ø—Ç–∞ ---
    if prompt_path and Path(prompt_path).exists():
        _append_md_section(md_lines, "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø—Ä–æ–º–ø—Ç–∞")
        prompt_text = Path(prompt_path).read_text(encoding="utf-8")
        md_lines.append("```text")
        md_lines.append(prompt_text)
        md_lines.append("```")

    # --- –°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ ---
    _append_md_section(md_lines, "–°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    md_lines.append("| –ö–ª—é—á | –ù–∞–∑–≤–∞–Ω–∏–µ |")
    md_lines.append("|------|----------|")
    for k, v in document_classes.items():
        md_lines.append(f"| {k} | {v} |")

    # --- –û–ø—Ä–µ–¥–µ–ª—è–µ–º run_id –ø–æ –∏–º–µ—é—â–∏–º—Å—è CSV-—Ñ–∞–π–ª–∞–º ---
    model_name_clean = model_cfg["model_name"].replace(" ", "_")
    prompt_name = Path(prompt_path).stem if prompt_path else "prompt"

    pattern = f"{model_name_clean}_{prompt_name}_*_final_classification_results.csv"
    candidate_files = sorted(Path(".").glob(pattern))
    if candidate_files:
        # –ë–µ—Ä—ë–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π (–ø–æ –∏–º–µ–Ω–∏, —Ç–∞–∫ –∫–∞–∫ timestamp –≤—Ö–æ–¥–∏—Ç –≤ –∏–º—è)
        latest_file = candidate_files[-1]
        run_id = latest_file.stem.replace("_final_classification_results", "")
    else:
        # –§–æ–ª–ª–±—ç–∫ ‚Äî –±–µ–∑ timestamp (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
        run_id = get_run_id(model_cfg["model_name"])  # type: ignore

    # --- –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ---
    final_metrics_file = Path(f"{run_id}_final_classification_results.csv")
    if final_metrics_file.exists():
        final_df = pd.read_csv(final_metrics_file)
        final_metrics = final_df.iloc[0].to_dict()
        _append_md_section(md_lines, "–ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
        md_lines.append("| Accuracy | F1-score | Precision | Recall |")
        md_lines.append("|----------|---------|-----------|--------|")
        md_lines.append(_metrics_row_to_md(final_metrics))

    # --- –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Å–∞–±—Å–µ—Ç–∞–º ---
    subset_metrics: List[Tuple[str, Dict[str, float]]] = []
    for subset in task_cfg.get("subsets", []):
        metrics_file = Path(f"{run_id}_{subset}_classification_results.csv")
        if metrics_file.exists():
            df = pd.read_csv(metrics_file)
            if not df.empty:
                subset_metrics.append((subset, df.iloc[0].to_dict()))

    if subset_metrics:
        _append_md_section(md_lines, "–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Å–∞–±—Å–µ—Ç–∞–º")
        md_lines.append("| –°–∞–±—Å–µ—Ç | Accuracy | F1-score | Precision | Recall |")
        md_lines.append("|--------|----------|---------|-----------|--------|")
        for subset, metrics in subset_metrics:
            row = _metrics_row_to_md(metrics)
            md_lines.append(f"| {subset} {row[1:]}")  # —É–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—ã–π —Å–∏–º–≤–æ–ª '|' —É row

    # --- –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (overall) ---
    overall_class_report = Path(f"{run_id}_overall_class_report.csv")
    if overall_class_report.exists():
        df_overall = pd.read_csv(overall_class_report, index_col=0)
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ precision/recall/F1 –∏ —É–±–∏—Ä–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É 'accuracy'
        df_overall = df_overall.drop(index=[row for row in ["accuracy"] if row in df_overall.index], errors="ignore")
        _append_md_section(md_lines, "–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º ‚Äî –æ–±—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç")
        md_lines.append(_df_to_md_table(df_overall))

    # --- –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∞–±—Å–µ—Ç–∞ ---
    for subset in task_cfg.get("subsets", []):
        class_rep_file = Path(f"{run_id}_{subset}_class_report.csv")
        if not class_rep_file.exists():
            continue
        df_subset = pd.read_csv(class_rep_file, index_col=0)
        df_subset = df_subset.drop(index=[row for row in ["accuracy"] if row in df_subset.index], errors="ignore")
        _append_md_section(md_lines, f"–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º ‚Äî {subset}")
        md_lines.append(_df_to_md_table(df_subset))

    # --- –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ ---
    for subset in task_cfg.get("subsets", []):
        cm_file = Path(f"{run_id}_{subset}_confusion_matrix.csv")
        if not cm_file.exists():
            continue
        cm_df = pd.read_csv(cm_file, index_col=0)
        _append_md_section(md_lines, f"Confusion Matrix ‚Äî {subset}")
        md_lines.append(_df_to_md_table(cm_df))

    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---
    output_path.write_text("\n".join(md_lines), encoding="utf-8")
    print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_path}")


# --- –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ ---
if __name__ == "__main__":
    CONFIG_PATH = Path("config_classification.json")

    # –°—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∏–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–º–ø—Ç–∞
    cfg = load_config(str(CONFIG_PATH))
    model_name_clean = cfg["model"]["model_name"].replace(" ", "_")
    prompt_name = Path(cfg["task"].get("prompt_path", "prompt")).stem
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    dyn_report_name = f"report_{model_name_clean}_{prompt_name}_{timestamp}.md"

    # –ï—Å–ª–∏ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å –≤ –∫–æ–Ω—Ñ–∏–≥–µ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π
    report_section = cfg.get("report", {}) if isinstance(cfg, dict) else {}
    OUTPUT_PATH = Path(report_section.get("output_path", dyn_report_name))

    build_report(CONFIG_PATH, OUTPUT_PATH)