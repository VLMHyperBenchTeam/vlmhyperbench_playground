import asyncio
import base64
import json
import os
import uuid
from asyncio import create_task
from pathlib import Path
from typing import Any, Dict

import click
import Levenshtein
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from dotenv import load_dotenv
from openai import AsyncOpenAI
from pydantic import BaseModel, create_model
from sklearn.metrics import f1_score, precision_score, recall_score
from tqdm.asyncio import tqdm

load_dotenv()

client = AsyncOpenAI(
    base_url=os.getenv("RUNPOD_URL"),
    api_key="token-test",
)


def char_error_rate(gt_str, pred_str):
    return Levenshtein.distance(gt_str, pred_str) / max(1, len(gt_str))


def word_error_rate(gt: str, pred: str) -> float:
    gt_words = gt.strip().split()
    pred_words = pred.strip().split()
    return Levenshtein.distance(" ".join(gt_words), " ".join(pred_words)) / max(
        1, len(gt_words)
    )


def evaluate(gt_path, pred_path, fuzzy_threshold=90):
    rows = []

    gt_path = Path(gt_path)
    pred_path = Path(pred_path)

    files = sorted(gt_path.glob("*.json"))

    for i, gt_file in enumerate(files):
        with open(gt_file, "r", encoding="utf-8") as f:
            gt = json.load(f)
        with open(pred_path / gt_file.name, "r", encoding="utf-8") as f:
            pred = json.load(f)

        for key in gt.keys():
            gt_val = gt.get(key, "").strip()
            pred_val = pred.get(key, "").strip()

            exact_match = int(gt_val == pred_val)

            cer = char_error_rate(gt_val, pred_val)
            wer = word_error_rate(gt_val, pred_val)

            rows.append(
                {
                    "doc_id": i,
                    "field": key,
                    "gt": gt_val,
                    "pred": pred_val,
                    "exact_match": exact_match,
                    "cer": cer,
                    "wer": wer,
                }
            )

    df = pd.DataFrame(rows)

    df["y_true"] = df["gt"] != ""
    df["y_pred"] = df["gt"] == df["pred"]

    precision = precision_score(df["y_true"], df["y_pred"])
    recall = recall_score(df["y_true"], df["y_pred"])
    f1 = f1_score(df["y_true"], df["y_pred"])

    exact_accuracy = df["exact_match"].mean()
    avg_cer = df["cer"].mean()
    avg_wer = df["wer"].mean()

    def compute_field_metrics(group):
        return pd.Series(
            {
                "exact_match": group["exact_match"].mean(),
                "cer": group["cer"].mean(),
                "wer": group["wer"].mean(),
                "precision": precision_score(
                    group["y_true"], group["y_pred"], zero_division=0
                ),
                "recall": recall_score(
                    group["y_true"], group["y_pred"], zero_division=0
                ),
                "f1": f1_score(group["y_true"], group["y_pred"], zero_division=0),
            }
        )

    per_field = df.groupby("field").apply(compute_field_metrics).reset_index()

    return {
        "exact_accuracy": exact_accuracy,
        "avg_cer": avg_cer,
        "avg_wer": avg_wer,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "per_field_metrics": per_field,
        "full_df": df,
    }


def plot_metrics(per_field_df):
    sns.set(style="whitegrid")

    plt.figure(figsize=(10, 5))
    sns.barplot(
        x="cer",
        y="field",
        data=per_field_df.sort_values("cer", ascending=False),
        palette="Reds_r",
    )
    plt.title("CER –ø–æ –ø–æ–ª—è–º")
    plt.xlabel("CER")
    plt.ylabel("–ü–æ–ª–µ")
    plt.tight_layout()
    plt.savefig("cer_per_field.png", dpi=300, bbox_inches="tight")

    # Fuzzy Accuracy –ø–æ –ø–æ–ª—è–º
    plt.figure(figsize=(10, 5))
    sns.barplot(
        x="fuzzy_match",
        y="field",
        data=per_field_df.sort_values("fuzzy_match", ascending=True),
        palette="Blues",
    )
    plt.title("Fuzzy Accuracy –ø–æ –ø–æ–ª—è–º")
    plt.xlabel("Fuzzy Accuracy")
    plt.ylabel("–ü–æ–ª–µ")
    plt.tight_layout()

    plt.savefig(
        "fuzzy_accurcy_per_field.png", dpi=300, bbox_inches="tight"
    )  # ‚Üê —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞

    # Exact Match Accuracy –ø–æ –ø–æ–ª—è–º
    plt.figure(figsize=(10, 5))
    sns.barplot(
        x="exact_match",
        y="field",
        data=per_field_df.sort_values("exact_match", ascending=True),
        palette="Greens",
    )
    plt.title("Exact Match Accuracy –ø–æ –ø–æ–ª—è–º")
    plt.xlabel("–¢–æ—á–Ω–æ—Å—Ç—å (Exact Match)")
    plt.ylabel("–ü–æ–ª–µ")
    plt.tight_layout()
    plt.savefig(
        "exact_match_accurcy_per_field.png", dpi=300, bbox_inches="tight"
    )  # ‚Üê —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞

    plt.figure(figsize=(12, 6))


def print_top_errors(per_field_df, top_n=5):
    print("\nüìâ –¢–æ–ø", top_n, "–ø–æ–ª–µ–π —Å —Å–∞–º—ã–º –≤—ã—Å–æ–∫–∏–º CER:")
    print(per_field_df.sort_values("cer", ascending=False).head(top_n))

    print("\n‚ùå –¢–æ–ø", top_n, "–ø–æ–ª–µ–π —Å —Å–∞–º–æ–π –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é (Exact Match):")
    print(per_field_df.sort_values("exact_match", ascending=True).head(top_n))


def generate_pydantic_model(
    json_data: Dict[str, Any], model_name: str = "ValidationGenerated"
) -> BaseModel:
    fields = {}

    def get_field_type(value: Any):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª—è."""
        if isinstance(value, bool):
            return bool
        elif isinstance(value, int):
            return int
        elif isinstance(value, float):
            return float
        elif isinstance(value, str):
            return str
        elif isinstance(value, list):
            # –î–ª—è —Å–ø–∏—Å–∫–æ–≤, —Å–æ–∑–¥–∞–µ–º —Ç–∏–ø List[Type]
            return list
        elif isinstance(value, dict):
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π
            return generate_pydantic_model(value, "NestedModel")
        else:
            return Any  # –î–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø–æ–ª—è –º–æ–¥–µ–ª–∏
    for key, value in json_data.items():
        fields[key] = (get_field_type(value), ...)

    # –°–æ–∑–¥–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å
    return create_model(model_name, **fields)


def read_prompt_from_file(filepath):
    with open(filepath, "r") as file:
        content = file.read()
    return content


def image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        # Encode the image as base64
        encoded_string = base64.b64encode(image_file.read())
        return encoded_string.decode("utf-8")


async def run_request_to_runpod(json_schema, base64_image, prompt, model_name):
    content = [
        {"type": "text", "text": prompt},
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
        },
    ]
    completion = await client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": content}],
        extra_body={"guided_json": json_schema},
    )
    return json.loads(completion.choices[0].message.content)


def read_json_file(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data


async def process_image(i, dataset_path, prompt, model_name):

    image = dataset_path / "images" / f"{i}.jpg"
    base64_image = image_to_base64(image)
    json_data = read_json_file(str(dataset_path / "jsons" / f"{i}.json"))
    GeneratedModel = generate_pydantic_model(json_data, "StructureModel")
    schema = GeneratedModel.model_json_schema()

    # –í—ã–∑–æ–≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    gt = await run_request_to_runpod(schema, base64_image, prompt, model_name)

    # –ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Ñ–∞–π–ª
    with open(
        Path("output") / dataset_path.name / "pred" / f"{i}.json", "w", encoding="utf-8"
    ) as f:
        json.dump(gt, f, ensure_ascii=False, indent=4)


async def check_entity_extractor(dataset_path, prompt_path, model_name, subsets):
    run_id = uuid.uuid4()
    subsets = [dataset_path / "images" / subset for subset in subsets]
    print(subsets)
    prompt = read_prompt_from_file(prompt_path)

    all_dfs = []
    all_field_metrics = []

    for subset in subsets:
        subset_name = subset.name
        print(f"\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–±—Å–µ—Ç–∞: {subset_name}")

        pred_dir = Path("output") / dataset_path.name / subset_name / "pred"
        pred_dir.mkdir(exist_ok=True, parents=True)

        image_files = sorted(list(subset.glob("*.jpg")))
        semaphore = asyncio.Semaphore(3)

        async def sem_task(i, *, _semaphore=semaphore, _image_files=image_files, _pred_dir=pred_dir):
            async with _semaphore:
                try:
                    image = _image_files[i]
                    image_id = image.stem
                    base64_image = image_to_base64(image)
                    json_data = read_json_file(
                        dataset_path / "jsons" / f"{image_id}.json"
                    )
                    GeneratedModel = generate_pydantic_model(
                        json_data, "StructureModel"
                    )
                    schema = GeneratedModel.model_json_schema()

                    gt = await run_request_to_runpod(
                        schema, base64_image, prompt, model_name
                    )

                    with open(
                        _pred_dir / f"{image_id}.json", "w", encoding="utf-8"
                    ) as f:
                        json.dump(gt, f, ensure_ascii=False, indent=4)
                except Exception as err:
                    print(err)

        tasks = [create_task(sem_task(i)) for i in range(len(image_files))]
        await tqdm.gather(*tasks)

        metrics = evaluate(dataset_path / "jsons", pred_dir)

        print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–∞–±—Å–µ—Ç–∞ {subset_name}:")
        print(f"Exact Match Accuracy: {metrics['exact_accuracy']:.4f}")
        print(f"Average CER: {metrics['avg_cer']:.4f}")
        print(f"Average WER: {metrics['avg_wer']:.4f}")
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall: {metrics['recall']:.4f}")
        print(f"F1-score: {metrics['f1']:.4f}")

        print_top_errors(metrics["per_field_metrics"])

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∞–±—Å–µ—Ç–µ
        metrics["full_df"]["subset"] = subset_name
        metrics["full_df"]["prompt"] = prompt
        metrics["per_field_metrics"]["subset"] = subset_name
        metrics["per_field_metrics"]["prompt"] = prompt

        all_dfs.append(metrics["full_df"])
        all_field_metrics.append(metrics["per_field_metrics"])

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
        metrics["full_df"].to_csv(
            f"{run_id}_{subset_name}_detailed_result.csv", index=False
        )
        metrics["per_field_metrics"].to_csv(
            f"{run_id}_{subset_name}_per_field_metrics.csv", index=False
        )

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    final_df = pd.concat(all_dfs, ignore_index=True)
    final_field_metrics = pd.concat(all_field_metrics, ignore_index=True)

    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º —Å–∞–±—Å–µ—Ç–∞–º
    overall_metrics = {
        "exact_accuracy": (final_df["gt"] == final_df["pred"]).mean(),
        "avg_cer": final_df["cer"].mean(),
        "avg_wer": final_df["wer"].mean(),
        "precision": precision_score(
            final_df["gt"] != "", final_df["gt"] == final_df["pred"]
        ),
        "recall": recall_score(
            final_df["gt"] != "", final_df["gt"] == final_df["pred"]
        ),
        "f1": f1_score(final_df["gt"] != "", final_df["gt"] == final_df["pred"]),
    }

    print("\nüìà –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º —Å–∞–±—Å–µ—Ç–∞–º:")
    for k, v in overall_metrics.items():
        print(f"{k}: {v:.4f}")

    final_df.to_csv(f"{run_id}_ALL_detailed_result.csv", index=False)
    final_field_metrics.to_csv(f"{run_id}_ALL_per_field_metrics.csv", index=False)


@click.command()
@click.option("--dataset-path", type=click.Path(path_type=Path))
@click.option("--prompt-path", type=click.Path(path_type=Path))
@click.option("--model-name", type=str)
@click.option(
    "--subsets",
    type=str,
    default=None,
    help="–°–ø–∏—Å–æ–∫ —Å–∞–±—Å–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –Ω–∞–ø—Ä–∏–º–µ—Ä: --subsets blur,noise,clean,bright,gray,rotated,spatter",
)
def main(dataset_path, prompt_path, model_name, subsets):
    if not subsets:
        subsets = [d.name for d in (dataset_path / "images").iterdir() if d.is_dir()]
    else:
        subsets = [s.strip() for s in subsets.split(",")]

    asyncio.run(check_entity_extractor(dataset_path, prompt_path, model_name, subsets))


if __name__ == "__main__":
    main()
